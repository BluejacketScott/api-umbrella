service:
  log_level: info
  # How often to flush inputs to the outputs.
  flush: <%- json_encode(config["fluent_bit"]["service"]["flush"]) %>
  storage.path: <%- json_encode(path_join(config["db_dir"], "fluent-bit")) %>
  storage.sync: normal
  storage.checksum: off
  # max_chunks_up * 2MB corresponds to roughly the maximum memory use before
  # things are buffered to disk for `storage.type: filesystem` inputs.
  storage.max_chunks_up: <%- json_encode(config["fluent_bit"]["service"]["storage_max_chunks_up"]) %>
  # Determine how many MB of data can be read from disk in the event fluent-bit
  # is restarted.
  storage.backlog.mem_limit: <%- json_encode(config["fluent_bit"]["service"]["storage_backlog_mem_limit"]) %>

pipeline:
  inputs:
    - name: fluentbit_metrics
      tag: internal_metrics
      scrape_interval: 60
      scrape_on_start: true

    - name: tcp
      tag: analytics.allowed
      listen: <%- json_encode(config["fluent_bit"]["host"]) %>
      port: <%- json_encode(config["fluent_bit"]["port"]) %>
      format: json
      storage.type: filesystem

    <% if config["log"]["destination"] == "console" then %>
    # Workaround for error.log currently being hard-coded to output to a file
    # that won't work if symlinked to /dev/stdout (it insists on rolling the file
    # since it doesn't think it can write to it).
    #
    # This workaround should no longer be needed once Trafficserver 10 is
    # released and things can be configured to output to stdout/stderr directly:
    # https://github.com/apache/trafficserver/pull/7937
    - name: tail
      tag: trafficserver
      path: <%- json_encode(path_join(config["log_dir"], "trafficserver/error.log")) %>
      refresh_interval: 5
    <% end %>

  filters:
    - name: rewrite_tag
      match: analytics.allowed
      rule: "$gatekeeper_denied_code ^. analytics.denied false"

    - name: rewrite_tag
      match: analytics.allowed
      rule: "$response_status ^[4-9] analytics.errored false"

  outputs:
    # Print what we would log to stdout for extra redundancy.
    - name: stdout
      match: "*"
      format: msgpack

    # Send API analytics to OpenSearch analytics DB.
    - name: opensearch
      match: "analytics.*"
      host: <%- json_encode(config["opensearch"]["_first_server"]["host"]) %>
      port: <%- json_encode(config["opensearch"]["_first_server"]["port"]) %>
      tls: <%- config["opensearch"]["_first_server"]["_https?"] and "on" or "off" %>
      <% if config["opensearch"]["_first_server"]["user"] then %>
      http_user: <%- json_encode(config["opensearch"]["_first_server"]["user"]) %>
      <% end %>
      <% if config["opensearch"]["_first_server"]["password"] then %>
      http_passwd: <%- json_encode(config["opensearch"]["_first_server"]["password"]) %>"
      <% end %>
      # aws_auth: on
      # aws_region: us-west-2
      index: <%- json_encode(config["opensearch"]["index_name_prefix"] .. "-logs-v" .. config["opensearch"]["template_version"] .. "-$TAG[1]") %>
      # Data streams require "create" operations.
      write_operation: create
      # _type field is no longer accepted for OpenSearch.
      suppress_type_name: on
      # Retry failed requests in the event the server is temporarily down.
      retry_limit: 30
      # Use our request ID for the document ID to help reduce the possibility
      # of duplicate data when retries are attempted (note that duplicate data
      # can still occur if the data stream index is rotated).
      id_key: request_id
      # Ensure the record is passed through without adding any extra metadata.
      logstash_format: off
      include_tag_key: off
      # Limit the on-disk buffer size.
      storage.total_limit_size: <%- json_encode(config["fluent_bit"]["outputs"]["storage_total_limit_size"]) %>
      # Read and report errors, increasing buffer size so more complete errors
      # can be read in.
      trace_error: on
      buffer_size: 16KB

    # - name: s3
    #   match: logs denied_logs
    #   compression: on
    #   upload_chunk_size: 30M
    #   total_file_size: 250M
    #   upload_timeout: 60m
    #   s3_key_format: "/$TAG[1]/%Y/%m/%d/%Y-%m-%dT%H:%M:%SZ-$UUID.gz
    #   send_content_md5: true
    #   auto_retry_requests: true
    #   preserve_data_ordering: true
    #   retry_limit: 5



