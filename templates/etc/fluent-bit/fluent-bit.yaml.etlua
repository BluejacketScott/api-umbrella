service:
  log_level: info
  # How often to flush inputs to the outputs.
  flush: <%- json_encode(config["fluent_bit"]["service"]["flush"]) %>
  storage.path: /tmp/fluentbit
  storage.sync: normal
  storage.checksum: off
  # max_chunks_up * 2MB corresponds to roughly the maximum memory use before
  # things are buffered to disk for `storage.type: filesystem` inputs.
  storage.max_chunks_up: <%- json_encode(config["fluent_bit"]["service"]["storage_max_chunks_up"]) %>
  # Determine how many MB of data can be read from disk in the event fluent-bit
  # is restarted.
  storage.backlog.mem_limit: <%- json_encode(config["fluent_bit"]["service"]["storage_backlog_mem_limit"]) %>

pipeline:
  inputs:
    - name: tcp
      tag: all
      listen: <%- json_encode(config["fluent_bit"]["host"]) %>
      port: <%- json_encode(config["fluent_bit"]["port"]) %>
      format: json
      storage.type: filesystem

    - name: fluentbit_metrics
      tag: internal_metrics
      scrape_interval: 60
      scrape_on_start: true

  filters:
    - name: rewrite_tag
      match: all
      rule: "$gatekeeper_denied_code ^. denied false"

  outputs:
    # Print what we would log to stdout for extra redundancy.
    - name: stdout
      match: "*"
      json_date_key: false
      format: json_lines

    # Send API logs to OpenSearch analytics DB.
    - name: opensearch
      match_regex: "^(all|denied)$"
      host: <%- json_encode(config["opensearch"]["_first_server"]["host"]) %>
      port: <%- json_encode(config["opensearch"]["_first_server"]["port"]) %>
      tls: <%- config["opensearch"]["_first_server"]["_https?"] and "on" or "off" %>
      <% if config["opensearch"]["_first_server"]["user"] then %>
      http_user: <%- json_encode(config["opensearch"]["_first_server"]["user"]) %>
      <% end %>
      <% if config["opensearch"]["_first_server"]["password"] then %>
      http_passwd: <%- json_encode(config["opensearch"]["_first_server"]["password"]) %>"
      <% end %>
      # aws_auth: on
      # aws_region: us-west-2
      index: <%- json_encode(config["opensearch"]["index_name_prefix"] .. "-logs-v" .. config["opensearch"]["template_version"] .. "-$TAG") %>
      # Data streams require "create" operations.
      write_operation: create
      # _type field is no longer accepted for OpenSearch.
      suppress_type_name: on
      # Retry failed requests in the event the server is temporarily down.
      retry_limit: 30
      # Use our request ID for the document ID to help reduce the possibility
      # of duplicate data when retries are attempted (note that duplicate data
      # can still occur if the data stream index is rotated).
      id_key: request_id
      # Ensure the record is passed through without adding any extra metadata.
      logstash_format: off
      include_tag_key: off
      # Limit the on-disk buffer size.
      storage.total_limit_size: <%- json_encode(config["fluent_bit"]["outputs"]["storage_total_limit_size"]) %>
      # Read and report errors, increasing buffer size so more complete errors
      # can be read in.
      trace_error: on
      buffer_size: 16KB

    # - name: s3
    #   match: logs denied_logs
    #   compression: on
    #   upload_chunk_size: 30M
    #   total_file_size: 250M
    #   upload_timeout: 60m
    #   s3_key_format: "/$TAG/%Y/%m/%d/%Y-%m-%dT%H:%M:%SZ-$UUID.gz
    #   send_content_md5: true
    #   auto_retry_requests: true
    #   preserve_data_ordering: true
    #   retry_limit: 5



